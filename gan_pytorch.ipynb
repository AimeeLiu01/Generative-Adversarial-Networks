{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matplotlib_is_available = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from matplotlib import pyplot as plt\n",
    "except ImportError:\n",
    "    print(\"Will skip plotting; matplotlib is not available.\")\n",
    "    matplotlib_is_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data [Only 4 moments]\n"
     ]
    }
   ],
   "source": [
    "# Data params\n",
    "data_mean = 4\n",
    "data_stddev = 1.25\n",
    "(name, preprocess, d_input_func) = (\"Only 4 moments\", lambda data: get_moments(data), lambda x: 4)\n",
    "print(\"Using data [%s]\" % (name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## DATA: Target data and generator input data\n",
    "def get_distribution_sampler(mu, sigma):\n",
    "    return lambda n: torch.Tensor(np.random.normal(mu, sigma, (1, n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_generator_input_sampler():\n",
    "    return lambda m, n: torch.rand(m, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, f):\n",
    "        super(Generator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "        self.f = f\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.map1(x)\n",
    "        x = self.f(x)\n",
    "        x = self.map2(x)\n",
    "        x = self.f(x)\n",
    "        x = self.map3(x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, f):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "        self.f = f\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.f(self.map1(x))\n",
    "        x = self.f(self.map2(x))\n",
    "        return self.f(self.map3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract(v):\n",
    "    return v.data.storage().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stats(d):\n",
    "    return [np.mean(d), np.std(d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_moments(d):\n",
    "    mean = torch.mean(d)\n",
    "    diffs = d - mean\n",
    "    var = torch.mean(torch.pow(diffs, 2.0))\n",
    "    std = torch.pow(var, 0.5)\n",
    "    zscores = diffs / std\n",
    "    skews = torch.mean(torch.pow(zscores, 3.0))\n",
    "    kurtoses = torch.mean(torch.pow(zscores, 4.0)) - 3.0\n",
    "    final = torch.cat((mean.reshape(1,), std.reshape(1,), skews.reshape(1,), kurtoses.reshape(1,)))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decorate_with_diffs(data, exponent, remove_raw_data=False):\n",
    "    mean = torch.mean(data.data, 1, keepdim=True)\n",
    "    mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[0][0])\n",
    "    diffs = torch.pow(data - Variable(mean_broadcast), exponent)\n",
    "    if remove_raw_data:\n",
    "        return torch.cat([diffs], 1)\n",
    "    else:\n",
    "        return torch.cat([data, diffs], 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Model parameters\n",
    "    g_input_size = 1      # Random noise dimension coming into generator, per output vector\n",
    "    g_hidden_size = 5     # Generator complexity\n",
    "    g_output_size = 1     # Size of generated output vector\n",
    "    d_input_size = 500    # Minibatch size - cardinality of distributions\n",
    "    d_hidden_size = 10    # Discriminator complexity\n",
    "    d_output_size = 1     # Single dimension for 'real' vs. 'fake' classification\n",
    "    minibatch_size = d_input_size\n",
    "\n",
    "    d_learning_rate = 1e-3\n",
    "    g_learning_rate = 1e-3\n",
    "    sgd_momentum = 0.9\n",
    "\n",
    "    num_epochs = 5000\n",
    "    print_interval = 100\n",
    "    d_steps = 20\n",
    "    g_steps = 20\n",
    "\n",
    "    dfe, dre, ge = 0, 0, 0\n",
    "    d_real_data, d_fake_data, g_fake_data = None, None, None\n",
    "\n",
    "    discriminator_activation_function = torch.sigmoid\n",
    "    generator_activation_function = torch.tanh\n",
    "\n",
    "    d_sampler = get_distribution_sampler(data_mean, data_stddev)\n",
    "    gi_sampler = get_generator_input_sampler()\n",
    "    G = Generator(input_size=g_input_size,\n",
    "                  hidden_size=g_hidden_size,\n",
    "                  output_size=g_output_size,\n",
    "                  f=generator_activation_function)\n",
    "    D = Discriminator(input_size=d_input_func(d_input_size),\n",
    "                      hidden_size=d_hidden_size,\n",
    "                      output_size=d_output_size,\n",
    "                      f=discriminator_activation_function)\n",
    "    criterion = nn.BCELoss()  # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
    "    d_optimizer = optim.SGD(D.parameters(), lr=d_learning_rate, momentum=sgd_momentum)\n",
    "    g_optimizer = optim.SGD(G.parameters(), lr=g_learning_rate, momentum=sgd_momentum)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for d_index in range(d_steps):\n",
    "            # 1. Train D on real+fake\n",
    "            D.zero_grad()\n",
    "\n",
    "            #  1A: Train D on real\n",
    "            d_real_data = Variable(d_sampler(d_input_size))\n",
    "            d_real_decision = D(preprocess(d_real_data))\n",
    "            d_real_error = criterion(d_real_decision, Variable(torch.ones([1,1])))  # ones = true\n",
    "            d_real_error.backward() # compute/store gradients, but don't change params\n",
    "\n",
    "            #  1B: Train D on fake\n",
    "            d_gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
    "            d_fake_data = G(d_gen_input).detach()  # detach to avoid training G on these labels\n",
    "            d_fake_decision = D(preprocess(d_fake_data.t()))\n",
    "            d_fake_error = criterion(d_fake_decision, Variable(torch.zeros([1,1])))  # zeros = fake\n",
    "            d_fake_error.backward()\n",
    "            d_optimizer.step()     # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
    "\n",
    "            dre, dfe = extract(d_real_error)[0], extract(d_fake_error)[0]\n",
    "\n",
    "        for g_index in range(g_steps):\n",
    "            # 2. Train G on D's response (but DO NOT train D on these labels)\n",
    "            G.zero_grad()\n",
    "\n",
    "            gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
    "            g_fake_data = G(gen_input)\n",
    "            dg_fake_decision = D(preprocess(g_fake_data.t()))\n",
    "            g_error = criterion(dg_fake_decision, Variable(torch.ones([1,1])))  # Train G to pretend it's genuine\n",
    "\n",
    "            g_error.backward()\n",
    "            g_optimizer.step()  # Only optimizes G's parameters\n",
    "            ge = extract(g_error)[0]\n",
    "\n",
    "        if epoch % print_interval == 0:\n",
    "            print(\"Epoch %s: D (%s real_err, %s fake_err) G (%s err); Real Dist (%s),  Fake Dist (%s) \" %\n",
    "                  (epoch, dre, dfe, ge, stats(extract(d_real_data)), stats(extract(d_fake_data))))\n",
    "\n",
    "    if matplotlib_is_available:\n",
    "        print(\"Plotting the generated distribution...\")\n",
    "        values = extract(g_fake_data)\n",
    "        print(\" Values: %s\" % (str(values)))\n",
    "        plt.hist(values, bins=50)\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Histogram of Generated Distribution')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchang/.conda/envs/conda_env/lib/python3.6/site-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: D (0.6889556646347046 real_err, 0.6957948803901672 fake_err) G (0.6910749673843384 err); Real Dist ([3.9499535445272924, 1.205199473562865]),  Fake Dist ([-0.16907907477021217, 0.018331445228219563]) \n",
      "Epoch 100: D (0.6677896976470947 real_err, 0.6273491978645325 fake_err) G (0.7208607196807861 err); Real Dist ([3.971503085464239, 1.247045495815234]),  Fake Dist ([4.116176860809326, 0.023772825364301378]) \n",
      "Epoch 200: D (0.6986449956893921 real_err, 0.7073771357536316 fake_err) G (0.7184908390045166 err); Real Dist ([3.970497629880905, 1.2568593559856818]),  Fake Dist ([7.545034315109253, 1.1785055618602505]) \n",
      "Epoch 300: D (0.6075584292411804 real_err, 0.621327817440033 fake_err) G (0.7428746223449707 err); Real Dist ([4.069549364030361, 1.2850664796863391]),  Fake Dist ([2.4998107914924623, 0.033094891080281304]) \n",
      "Epoch 400: D (0.5656082034111023 real_err, 0.6463292241096497 fake_err) G (0.744191586971283 err); Real Dist ([3.93176224899292, 1.1676744680893782]),  Fake Dist ([8.145289655685424, 1.3120059870358314]) \n",
      "Epoch 500: D (0.5024309158325195 real_err, 0.580387532711029 fake_err) G (0.8272752165794373 err); Real Dist ([4.018569875533693, 1.2415065002849734]),  Fake Dist ([5.696604679107666, 0.696901639288058]) \n",
      "Epoch 600: D (0.31145402789115906 real_err, 0.21314649283885956 fake_err) G (1.372714638710022 err); Real Dist ([3.9389591096639633, 1.1939239813551508]),  Fake Dist ([-2.7926615104675294, 1.0742927959807906]) \n",
      "Epoch 700: D (0.2288518249988556 real_err, 0.20811602473258972 fake_err) G (1.4041664600372314 err); Real Dist ([4.086310267150402, 1.2553634667850826]),  Fake Dist ([0.9262110031545162, 4.399561038475961]) \n",
      "Epoch 800: D (0.7071484327316284 real_err, 0.6993172764778137 fake_err) G (0.6859529614448547 err); Real Dist ([3.965858996629715, 1.2407497322105363]),  Fake Dist ([5.455896995544434, 0.21305189207546898]) \n",
      "Epoch 900: D (0.502543032169342 real_err, 0.27134203910827637 fake_err) G (0.8845704197883606 err); Real Dist ([3.9322445674240587, 1.2130025423279072]),  Fake Dist ([4.45152602815628, 1.345845677192035]) \n",
      "Epoch 1000: D (0.7176265716552734 real_err, 0.7220691442489624 fake_err) G (0.6605480909347534 err); Real Dist ([4.011693791627884, 1.2552567050047803]),  Fake Dist ([5.07372145241499, 1.737416872846164]) \n",
      "Epoch 1100: D (0.6953598856925964 real_err, 0.6954964399337769 fake_err) G (0.6903830766677856 err); Real Dist ([4.020563279032707, 1.2111663067645857]),  Fake Dist ([5.855092149972916, 2.0323267745147557]) \n",
      "Epoch 1200: D (0.6925948262214661 real_err, 0.69281405210495 fake_err) G (0.6939560174942017 err); Real Dist ([4.110393023848534, 1.234250624054225]),  Fake Dist ([5.016951925754547, 1.5373558170108566]) \n",
      "Epoch 1300: D (0.6918542385101318 real_err, 0.6954855918884277 fake_err) G (0.6913201808929443 err); Real Dist ([4.042999257564545, 1.2184303348489012]),  Fake Dist ([3.104610748529434, 1.5432942017552427]) \n",
      "Epoch 1400: D (0.6948252320289612 real_err, 0.6936370134353638 fake_err) G (0.6908424496650696 err); Real Dist ([4.028870376110077, 1.2648439857251332]),  Fake Dist ([4.283596198618412, 1.8974480825571698]) \n",
      "Epoch 1500: D (0.6947695016860962 real_err, 0.6930185556411743 fake_err) G (0.6936041116714478 err); Real Dist ([4.119977009534836, 1.2421072974128398]),  Fake Dist ([4.571415984094143, 1.855536096019752]) \n",
      "Epoch 1600: D (0.6928066611289978 real_err, 0.6929967403411865 fake_err) G (0.6927507519721985 err); Real Dist ([3.94329870223999, 1.2404721303504544]),  Fake Dist ([4.106012953102589, 1.7631064865832267]) \n",
      "Epoch 1700: D (0.6918383836746216 real_err, 0.6935921907424927 fake_err) G (0.6929492950439453 err); Real Dist ([3.9458769426345826, 1.2051250144597694]),  Fake Dist ([3.616099235415459, 1.6708900911850133]) \n",
      "Epoch 1800: D (0.6940315961837769 real_err, 0.6915441751480103 fake_err) G (0.6939172148704529 err); Real Dist ([3.975920481622219, 1.2549088010651384]),  Fake Dist ([3.873924391269684, 1.7393842006139266]) \n",
      "Epoch 1900: D (0.7001014947891235 real_err, 0.6913487315177917 fake_err) G (0.694598376750946 err); Real Dist ([4.065364844787866, 1.2809389167797716]),  Fake Dist ([3.9411185373663904, 1.7179635135042297]) \n",
      "Epoch 2000: D (0.686539351940155 real_err, 0.6872830986976624 fake_err) G (0.6960999369621277 err); Real Dist ([3.9945960359573363, 1.2041121147539455]),  Fake Dist ([3.966893764734268, 1.566485860970756]) \n",
      "Epoch 2100: D (0.6952666640281677 real_err, 0.6979935765266418 fake_err) G (0.6899673342704773 err); Real Dist ([3.9634250051528217, 1.2617579401073262]),  Fake Dist ([4.112414270281792, 1.152061845294956]) \n",
      "Epoch 2200: D (0.6962370872497559 real_err, 0.696678638458252 fake_err) G (0.6896654367446899 err); Real Dist ([3.9520143432617187, 1.2101295615853611]),  Fake Dist ([4.058895664453506, 0.9552591485168006]) \n",
      "Epoch 2300: D (0.693717360496521 real_err, 0.6943919062614441 fake_err) G (0.6919853091239929 err); Real Dist ([4.047301634967327, 1.2296547474932547]),  Fake Dist ([4.201845112800598, 0.9387998599831977]) \n",
      "Epoch 2400: D (0.6938893795013428 real_err, 0.6934074759483337 fake_err) G (0.6928866505622864 err); Real Dist ([4.027503805756569, 1.299878382534571]),  Fake Dist ([4.3819033222198485, 0.9219983111344517]) \n",
      "Epoch 2500: D (0.6923871636390686 real_err, 0.6930149793624878 fake_err) G (0.6935691833496094 err); Real Dist ([3.996891320705414, 1.2559957757508533]),  Fake Dist ([4.491223199367523, 0.9418435877691097]) \n",
      "Epoch 2600: D (0.6941553950309753 real_err, 0.6916155815124512 fake_err) G (0.6941280364990234 err); Real Dist ([4.009564144730568, 1.2525242935488599]),  Fake Dist ([4.557818367004394, 0.9837833184386863]) \n",
      "Epoch 2700: D (0.6910778284072876 real_err, 0.6910891532897949 fake_err) G (0.6958069801330566 err); Real Dist ([3.9990186338424683, 1.3097879783338437]),  Fake Dist ([4.4956701836586, 0.9922302151251939]) \n",
      "Epoch 2800: D (0.6905606985092163 real_err, 0.693295955657959 fake_err) G (0.6949835419654846 err); Real Dist ([4.004571582794189, 1.272966819001648]),  Fake Dist ([4.084036037445069, 1.1346484619255912]) \n",
      "Epoch 2900: D (0.6982454061508179 real_err, 0.6945760846138 fake_err) G (0.6908543109893799 err); Real Dist ([3.999168781161308, 1.2446623062691338]),  Fake Dist ([3.6371439995765686, 1.290139623523704]) \n",
      "Epoch 3000: D (0.6918790936470032 real_err, 0.694429874420166 fake_err) G (0.6924493312835693 err); Real Dist ([4.005645015954971, 1.2624957444932348]),  Fake Dist ([3.8664425251483916, 1.3986516165955944]) \n",
      "Epoch 3100: D (0.692943811416626 real_err, 0.6939613819122314 fake_err) G (0.6921558380126953 err); Real Dist ([3.909467261493206, 1.2769513362318403]),  Fake Dist ([3.708816792845726, 1.4368086071006603]) \n",
      "Epoch 3200: D (0.6930336952209473 real_err, 0.6938877701759338 fake_err) G (0.6937696933746338 err); Real Dist ([4.04272039091587, 1.2057948769636184]),  Fake Dist ([3.685635720014572, 1.4471275123680631]) \n",
      "Epoch 3300: D (0.6923875212669373 real_err, 0.6934939026832581 fake_err) G (0.6935250163078308 err); Real Dist ([4.009513034671545, 1.2553713362542585]),  Fake Dist ([3.867230313360691, 1.442338412932885]) \n",
      "Epoch 3400: D (0.6923456788063049 real_err, 0.6930577754974365 fake_err) G (0.693418025970459 err); Real Dist ([3.9841229079961775, 1.2743401173304592]),  Fake Dist ([3.950006242096424, 1.3606198298171632]) \n",
      "Epoch 3500: D (0.6920226216316223 real_err, 0.6931387186050415 fake_err) G (0.69309401512146 err); Real Dist ([3.9536921217152847, 1.3079916213557032]),  Fake Dist ([3.9819243002533913, 1.3587646823180364]) \n",
      "Epoch 3600: D (0.6920613050460815 real_err, 0.6934940218925476 fake_err) G (0.6928413510322571 err); Real Dist ([3.977666113615036, 1.3062835962574681]),  Fake Dist ([3.9126852508187295, 1.2540529263462576]) \n",
      "Epoch 3700: D (0.6947551369667053 real_err, 0.6938042640686035 fake_err) G (0.693222165107727 err); Real Dist ([4.012139298550784, 1.2755832830425444]),  Fake Dist ([4.005288430690765, 1.230699296143018]) \n",
      "Epoch 3800: D (0.6930521726608276 real_err, 0.6930187940597534 fake_err) G (0.6927357316017151 err); Real Dist ([3.9863841927023604, 1.2508924140742361]),  Fake Dist ([4.056910691857338, 1.112724647463505]) \n",
      "Epoch 3900: D (0.6932144165039062 real_err, 0.6936827898025513 fake_err) G (0.6932820081710815 err); Real Dist ([4.025092971801758, 1.2380219763513471]),  Fake Dist ([4.177171374797821, 1.144191738364496]) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4000: D (0.6933771967887878 real_err, 0.6933819055557251 fake_err) G (0.6931486129760742 err); Real Dist ([3.9910437933020293, 1.2722911980794158]),  Fake Dist ([4.068264288783073, 1.1722103620263922]) \n",
      "Epoch 4100: D (0.6930574178695679 real_err, 0.693240761756897 fake_err) G (0.6930882930755615 err); Real Dist ([4.040175447762013, 1.2881250487411156]),  Fake Dist ([4.1143973183631894, 1.1519401649283845]) \n",
      "Epoch 4200: D (0.6933280825614929 real_err, 0.692965030670166 fake_err) G (0.6941720843315125 err); Real Dist ([3.9774863047599793, 1.2965168878664872]),  Fake Dist ([4.0912395415306095, 1.161782345054457]) \n",
      "Epoch 4300: D (0.6931016445159912 real_err, 0.6930104494094849 fake_err) G (0.6932520866394043 err); Real Dist ([3.922441308438778, 1.2070770591637268]),  Fake Dist ([4.1222873734235765, 1.1749606950750573]) \n",
      "Epoch 4400: D (0.6937932968139648 real_err, 0.6925145983695984 fake_err) G (0.6926584243774414 err); Real Dist ([4.077477504462004, 1.2484844425206272]),  Fake Dist ([3.951195772886276, 1.2149472581528022]) \n",
      "Epoch 4500: D (0.6929618120193481 real_err, 0.692703127861023 fake_err) G (0.6933810114860535 err); Real Dist ([4.015029375553131, 1.215823975400175]),  Fake Dist ([4.029778304576873, 1.295801163599061]) \n",
      "Epoch 4600: D (0.6937739849090576 real_err, 0.6934130787849426 fake_err) G (0.6928288340568542 err); Real Dist ([4.043774192124605, 1.2770409054812453]),  Fake Dist ([4.015139384627342, 1.286705338477258]) \n",
      "Epoch 4700: D (0.6933583617210388 real_err, 0.6936062574386597 fake_err) G (0.692711353302002 err); Real Dist ([4.035845472037792, 1.2466195887091593]),  Fake Dist ([3.880252792775631, 1.27077437304954]) \n",
      "Epoch 4800: D (0.6928525567054749 real_err, 0.6934439539909363 fake_err) G (0.6928344368934631 err); Real Dist ([3.9834988465309142, 1.2848697134117029]),  Fake Dist ([3.9897511999607085, 1.2729016075620387]) \n",
      "Epoch 4900: D (0.692925214767456 real_err, 0.6931488513946533 fake_err) G (0.6928866505622864 err); Real Dist ([3.9897216806411744, 1.2234451390065553]),  Fake Dist ([3.9606561248898506, 1.3370516405127872]) \n",
      "Plotting the generated distribution...\n",
      " Values: [5.949092388153076, 5.726511001586914, 3.022463321685791, 3.9460532665252686, 3.141266345977783, 2.4584789276123047, 3.5042760372161865, 3.619838237762451, 4.083578109741211, 3.2829806804656982, 3.2478151321411133, 3.627044677734375, 3.8906753063201904, 5.386401176452637, 3.6357643604278564, 0.6640535593032837, 3.934617280960083, 6.246848106384277, 3.3753321170806885, 6.0648956298828125, 0.7334020137786865, 3.9282655715942383, 5.214279651641846, 4.8377685546875, 4.55721378326416, 3.4407196044921875, 3.610438346862793, 4.579427719116211, 1.2815918922424316, 3.2592966556549072, 3.4866786003112793, 3.4059879779815674, 3.2797083854675293, 5.210253715515137, 5.635459899902344, 1.2178122997283936, 0.3480111360549927, 3.254589080810547, 3.733660936355591, 4.39892578125, 6.113566875457764, 6.346112251281738, 4.784766674041748, 5.725184440612793, 3.5556883811950684, 6.16639518737793, 3.57248592376709, 2.287511110305786, 3.7164113521575928, 1.9398915767669678, 2.157085657119751, 3.5046346187591553, 4.5624542236328125, 6.580011367797852, 3.9141509532928467, 5.696072101593018, 3.450455665588379, 2.6662185192108154, 5.2284650802612305, 3.7484896183013916, 5.255053997039795, 3.721555709838867, 4.982314109802246, 4.078236103057861, 3.785165548324585, 4.602238178253174, 2.9501793384552, 3.894407033920288, 3.3209383487701416, 3.7156012058258057, 3.3738853931427, 4.054110527038574, 4.372979640960693, 2.0649454593658447, 3.277172327041626, 4.539484977722168, 3.165212631225586, 3.7186899185180664, 4.140092372894287, 5.260158538818359, 5.194681167602539, 3.503074884414673, 3.735136032104492, 5.711182594299316, 4.121547222137451, 5.336343288421631, 1.514634609222412, 3.620584487915039, 2.536309242248535, 4.159002780914307, 6.5356059074401855, 3.478121042251587, 4.40391206741333, 4.300269603729248, 5.270462512969971, 0.5205308794975281, 6.501735687255859, 4.082860469818115, 4.443066596984863, 6.614155292510986, 3.592160224914551, 0.6987711787223816, 3.1434812545776367, 3.681626319885254, 3.145977258682251, 4.012866020202637, 3.7116434574127197, 3.883941411972046, 5.1453680992126465, 3.9231045246124268, 2.011127471923828, 2.9970591068267822, 3.7924253940582275, 4.455984115600586, 3.323066234588623, 4.480640411376953, 2.733128547668457, 3.6078929901123047, 3.3286924362182617, 3.7516775131225586, 3.2274560928344727, 2.7696292400360107, 4.195866107940674, 5.1991071701049805, 4.39933443069458, 6.0260419845581055, 4.122849941253662, 3.8145744800567627, 5.142172813415527, 4.019125461578369, 6.352555274963379, 4.661016464233398, 3.677753448486328, 3.4099314212799072, 3.432804584503174, 2.975792169570923, 4.932345390319824, 2.9394237995147705, 6.206020355224609, 4.810977935791016, 6.168895244598389, 6.57666540145874, 6.645061492919922, 3.5644168853759766, 1.560349702835083, 3.1460509300231934, 4.955174446105957, 3.758269786834717, 4.713293552398682, 6.465511322021484, 5.620938777923584, 5.168146133422852, 6.258673191070557, 4.303164482116699, 3.58274507522583, 2.730299949645996, 5.375639915466309, 5.5022196769714355, 5.312388896942139, 6.4720377922058105, 4.283684253692627, 3.75860333442688, 3.4583048820495605, 5.589653015136719, 5.026538372039795, 3.4805445671081543, 2.992231845855713, 3.7190492153167725, 3.3044731616973877, 2.9646849632263184, 4.664063453674316, 3.174795627593994, 4.2167768478393555, 3.305312156677246, 3.3787872791290283, 5.765111446380615, 3.002528429031372, 6.111508846282959, 3.5091214179992676, 3.6485962867736816, 3.488651752471924, 4.078873157501221, 3.1106410026550293, 3.833966016769409, 4.03166389465332, 3.2239112854003906, 3.364356517791748, 2.9381728172302246, 2.9920666217803955, 3.4230575561523438, 3.852076530456543, 6.4558281898498535, 3.99104642868042, 2.3257293701171875, 2.9118971824645996, 3.0435245037078857, 3.4187264442443848, 3.5435373783111572, 4.375235557556152, 3.4713473320007324, 2.2800261974334717, 5.070751190185547, 2.6428136825561523, 5.4873809814453125, 3.6076443195343018, 1.7158446311950684, 3.7330782413482666, 4.048877239227295, 2.0407276153564453, 3.099591016769409, 3.445643424987793, 4.684595108032227, 6.221963405609131, 6.08941650390625, 6.3441081047058105, 4.55897331237793, 4.311411380767822, 3.3181254863739014, 0.6324595808982849, 3.5854737758636475, 2.84859037399292, 2.3481593132019043, 0.44526660442352295, 5.725009441375732, 2.0775492191314697, 2.0276734828948975, 3.6782355308532715, 3.1408491134643555, 4.347038269042969, 4.4158501625061035, 4.340629577636719, 3.5351579189300537, 4.0218353271484375, 3.5854907035827637, 3.13214111328125, 0.9349662065505981, 2.3165793418884277, 3.474656820297241, 4.937493801116943, 3.2832236289978027, 3.10670804977417, 3.6957311630249023, 5.448709487915039, 2.367530345916748, 3.617055892944336, 2.182856798171997, 3.8926079273223877, 3.2894198894500732, 5.6715922355651855, 5.079215049743652, 6.636484622955322, 2.498154401779175, 4.219141960144043, 3.6183695793151855, 3.051483631134033, 4.42450475692749, 3.3603153228759766, 5.927263259887695, 6.439740180969238, 3.756526231765747, 4.10555362701416, 1.0398355722427368, 6.059469699859619, 3.499054431915283, 3.381500482559204, 1.2482178211212158, 1.5651257038116455, 3.2563161849975586, 3.434213161468506, 3.893622875213623, 1.4094109535217285, 6.186338901519775, 3.3357458114624023, 3.0454487800598145, 4.397770881652832, 6.388771057128906, 3.715075969696045, 3.3262641429901123, 6.576333045959473, 3.7444307804107666, 3.530548572540283, 4.931175708770752, 6.320958614349365, 2.117847442626953, 4.363373279571533, 3.9554593563079834, 5.899953842163086, 4.318785667419434, 6.277016639709473, 3.5717921257019043, 5.744753837585449, 5.400439262390137, 3.1168911457061768, 2.277650833129883, 3.4154934883117676, 0.8477494716644287, 3.434760093688965, 3.327810525894165, 2.355466842651367, 3.092546224594116, 0.5463125705718994, 3.4769210815429688, 6.347810745239258, 4.315621376037598, 4.09090518951416, 4.649112224578857, 2.640927791595459, 3.6816768646240234, 3.6747608184814453, 4.21696662902832, 3.723667621612549, 6.453254699707031, 5.7555646896362305, 3.143609046936035, 0.9953566789627075, 3.298253297805786, 3.4495158195495605, 0.5564006567001343, 5.58098840713501, 4.113749027252197, 5.171332836151123, 3.0879435539245605, 3.9302191734313965, 3.559950828552246, 3.5563604831695557, 4.765875339508057, 3.4326295852661133, 3.5898733139038086, 4.364014148712158, 3.596740484237671, 4.3968963623046875, 6.091975212097168, 2.323324203491211, 3.9094080924987793, 3.4107799530029297, 4.6799726486206055, 6.074374198913574, 0.3352513909339905, 4.199376583099365, 3.2545909881591797, 2.5553605556488037, 0.45069143176078796, 2.4309301376342773, 3.1322879791259766, 3.595278263092041, 3.562727928161621, 3.9590535163879395, 5.1554059982299805, 3.5992355346679688, 5.954664707183838, 5.707745552062988, 4.271539211273193, 3.944465160369873, 3.5462372303009033, 2.6986312866210938, 2.7098584175109863, 4.970426559448242, 3.189148426055908, 0.9873842597007751, 4.741213798522949, 6.638575553894043, 4.792264938354492, 3.424705982208252, 4.049065589904785, 2.850675582885742, 2.4485905170440674, 4.370832443237305, 3.6746058464050293, 3.3274641036987305, 3.2153048515319824, 3.026183605194092, 3.3066627979278564, 3.1797313690185547, 0.862825870513916, 3.3934133052825928, 2.643033266067505, 4.099099636077881, 3.94040584564209, 4.63438081741333, 5.962154388427734, 4.744117736816406, 3.0627968311309814, 4.801937580108643, 6.472114086151123, 1.2004437446594238, 3.9044320583343506, 4.1823344230651855, 6.650782585144043, 3.520064115524292, 2.936321258544922, 3.6345014572143555, 4.696367263793945, 6.575300693511963, 3.5611720085144043, 3.5400638580322266, 3.1753196716308594, 6.310583114624023, 5.80592155456543, 0.7759716510772705, 3.3162591457366943, 4.169670104980469, 4.328259468078613, 0.500526487827301, 3.4601285457611084, 3.368178367614746, 3.393178939819336, 3.659609794616699, 3.2508435249328613, 3.8464913368225098, 0.40434694290161133, 1.7609069347381592, 4.3745622634887695, 5.600925922393799, 5.415660858154297, 5.989684104919434, 1.0379202365875244, 4.445440769195557, 3.798464775085449, 3.454315423965454, 3.5073952674865723, 6.1791887283325195, 4.783702850341797, 3.960918426513672, 4.672934055328369, 4.5384674072265625, 3.6125922203063965, 3.3178157806396484, 3.368692636489868, 5.721993446350098, 4.758515357971191, 3.954279661178589, 4.646641254425049, 3.4997920989990234, 3.4841556549072266, 4.105001449584961, 3.453162431716919, 1.544358491897583, 3.0988454818725586, 3.550314426422119, 3.493616819381714, 1.3594294786453247, 3.5288519859313965, 4.045757293701172, 3.689846992492676, 5.253423690795898, 5.1040849685668945, 6.008145332336426, 5.400156021118164, 2.0841751098632812, 3.732576370239258, 3.17848539352417, 2.0622735023498535, 3.5037498474121094, 3.0086774826049805, 3.8672635555267334, 3.590982437133789, 2.4152772426605225, 3.632547378540039, 1.1075351238250732, 3.348433017730713, 3.1090900897979736, 6.584502220153809, 6.041043758392334, 4.437836647033691, 3.106475353240967, 4.60543966293335, 5.946270942687988, 2.923205852508545, 3.711113929748535, 4.205504894256592, 5.2169976234436035, 5.651186943054199, 2.774763345718384, 6.627752304077148, 6.39076042175293, 3.6507763862609863, 6.627168655395508, 3.4432363510131836, 3.2479071617126465, 3.7668330669403076, 3.3100380897521973, 3.3836166858673096, 3.453226089477539, 5.475533485412598, 4.524184226989746, 6.1615376472473145, 6.587358474731445, 1.5505573749542236, 2.2093491554260254, 2.437971830368042, 3.589524745941162, 5.303420066833496, 3.351407527923584, 3.64375901222229, 6.045727252960205, 4.934797286987305, 4.381685733795166, 3.27414608001709, 3.0467703342437744, 3.473691701889038]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115cdcac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
